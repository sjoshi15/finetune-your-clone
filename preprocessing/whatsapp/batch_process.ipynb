{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "folder_fp = \"./\"\n",
    "folder_path = Path(folder_fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhatsApp Chat with Jeremiah 822.zip\n",
      "WhatsApp Chat with Yiming.zip\n",
      "WhatsApp Chat with Pang Junjie.zip\n",
      "WhatsApp Chat with Thong Loon.zip\n",
      "WhatsApp Chat with Zhuang Qiang.zip\n",
      "WhatsApp Chat with Wan Ching.zip\n",
      "WhatsApp Chat with Wenjie Boon.zip\n",
      "WhatsApp Chat with David.zip\n",
      "WhatsApp Chat with Minghui.zip\n",
      "WhatsApp Chat with King Sen.zip\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "for zip_path in folder_path.glob('*.zip'):\n",
    "    print(zip_path)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('./')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watsonchua/environments/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning:\n",
      "\n",
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "/home/watsonchua/environments/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning:\n",
      "\n",
      "`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from load_whatsapp import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_path = \"./data/WhatsApp Chat with Wan Ching/WhatsApp Chat with Wan Ching.txt\" \n",
    "output_path = \"./data/WhatsApp Chat with Wan Ching/wa_finetune_2.jsonl\"\n",
    "role = \"wife\"\n",
    "main(input_path, output_path, role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1934/1934 [00:00<00:00, 12268.39it/s]\n",
      "100%|██████████| 1073/1073 [00:00<00:00, 4140.57it/s]\n",
      "100%|██████████| 6851/6851 [00:00<00:00, 11470.10it/s]\n",
      "100%|██████████| 3714/3714 [00:00<00:00, 4313.11it/s]\n",
      "100%|██████████| 12717/12717 [00:01<00:00, 11639.42it/s]\n",
      "100%|██████████| 6306/6306 [00:01<00:00, 4591.35it/s]\n",
      "100%|██████████| 2420/2420 [00:00<00:00, 13262.43it/s]\n",
      "100%|██████████| 1532/1532 [00:00<00:00, 4208.80it/s]\n",
      "100%|██████████| 3360/3360 [00:00<00:00, 13697.98it/s]\n",
      "100%|██████████| 1731/1731 [00:00<00:00, 4427.53it/s]\n",
      "  0%|          | 0/4136 [00:00<?, ?it/s]/home/watsonchua/work/llm-experiments/whatsapp/data/../load_whatsapp.py:31: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "100%|██████████| 4136/4136 [00:00<00:00, 12407.25it/s]\n",
      "100%|██████████| 2196/2196 [00:00<00:00, 3737.82it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 12240.76it/s]\n",
      "100%|██████████| 316/316 [00:00<00:00, 3994.62it/s]\n",
      "100%|██████████| 5256/5256 [00:00<00:00, 10874.91it/s]\n",
      "100%|██████████| 2936/2936 [00:00<00:00, 4617.30it/s]\n",
      "100%|██████████| 6278/6278 [00:00<00:00, 13572.92it/s]\n",
      "100%|██████████| 3658/3658 [00:00<00:00, 4184.11it/s]\n"
     ]
    }
   ],
   "source": [
    "for input_path in folder_path.glob('WhatsApp Chat*.txt'):\n",
    "    if str(input_path).endswith('Wan Ching.txt'):\n",
    "        continue\n",
    "    output_path = input_path.stem.replace('WhatsApp Chat with ', 'wa_finetune_').replace(' ', '_').lower() + '.jsonl'\n",
    "    main(input_path, output_path, \"friend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wa_finetune_david.jsonl\n",
      "wa_finetune_wenjie_boon.jsonl\n",
      "wa_finetune_minghui.jsonl\n",
      "wa_finetune_yiming.jsonl\n",
      "wa_finetune_wan_ching.jsonl\n",
      "wa_finetune_zhuang_qiang.jsonl\n",
      "wa_finetune_king_sen.jsonl\n",
      "wa_finetune_thong_loon.jsonl\n",
      "wa_finetune_pang_junjie.jsonl\n",
      "wa_finetune_jeremiah_822.jsonl\n"
     ]
    }
   ],
   "source": [
    "all_content = \"\"\n",
    "for input_path in folder_path.glob('wa_finetune_*.jsonl'):\n",
    "    print(input_path)\n",
    "    all_content += input_path.read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./consolidated_wa_finetune.jsonl', 'w') as f:\n",
    "    f.write(all_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from json import JSONDecodeError\n",
    "# check if they are all json loadable\n",
    "with open('./consolidated_wa_finetune.jsonl', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for idx, l in enumerate(lines):\n",
    "    try:\n",
    "        json.loads(l)\n",
    "    except JSONDecodeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
